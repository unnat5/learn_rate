{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/unnatsingh/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5393920141694561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1,6,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(X_train[i],cmap=\"gray\")\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OnehotLabel\n",
    "import pandas as pd\n",
    "y_train_df = pd.get_dummies(y_train)\n",
    "y_train=np.array(pd.get_dummies(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Reshaping X_train\n",
    "X_train_matrix = X_train.reshape(60000,-1)\n",
    "X_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 20)                280       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model in Lecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Dense\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(20,input_dim=x_train.shape[1],activation=\"relu\"))\n",
    "#model1.add(Dense(6,activation=\"relu\"))\n",
    "#model1.add(Dense(1,activation=\"relu\"))\n",
    "#model1.add(Dense(10,activation=\"relu\"))\n",
    "model1.add(Dense(1))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1)\n",
    "y_test_one_hot = np.array(pd.get_dummies(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0e22b5d85df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model1.compile(loss=\"mean_squared_error\",optimizer=\"sgd\",metrics=[\"accuracy\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#score_1 = model1.evaluate(x_test,y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#100*score_1[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "model1.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "#model1.compile(loss=\"mean_squared_error\",optimizer=\"sgd\",metrics=[\"accuracy\"])\n",
    "#score_1 = model1.evaluate(x_test,y_test)\n",
    "#100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 4791.8574\n",
      "Epoch 00001: val_loss improved from inf to 3051.34692, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 1s 2ms/step - loss: 4668.1861 - val_loss: 3051.3469\n",
      "Epoch 2/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 4045.8965\n",
      "Epoch 00002: val_loss improved from 3051.34692 to 2212.76758, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 3647.7879 - val_loss: 2212.7676\n",
      "Epoch 3/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 3197.0300\n",
      "Epoch 00003: val_loss improved from 2212.76758 to 1598.29102, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 2795.9917 - val_loss: 1598.2910\n",
      "Epoch 4/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1609.3440\n",
      "Epoch 00004: val_loss improved from 1598.29102 to 1180.34485, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 141us/step - loss: 2155.3319 - val_loss: 1180.3448\n",
      "Epoch 5/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1954.8530\n",
      "Epoch 00005: val_loss improved from 1180.34485 to 921.80499, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 150us/step - loss: 1732.9157 - val_loss: 921.8050\n",
      "Epoch 6/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 923.2890\n",
      "Epoch 00006: val_loss improved from 921.80499 to 778.58795, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 169us/step - loss: 1428.0752 - val_loss: 778.5880\n",
      "Epoch 7/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1395.4740\n",
      "Epoch 00007: val_loss improved from 778.58795 to 704.34198, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 1270.2735 - val_loss: 704.3420\n",
      "Epoch 8/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1142.4644\n",
      "Epoch 00008: val_loss improved from 704.34198 to 662.66931, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 177us/step - loss: 1154.0669 - val_loss: 662.6693\n",
      "Epoch 9/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1160.8351\n",
      "Epoch 00009: val_loss improved from 662.66931 to 628.54114, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 1079.0089 - val_loss: 628.5411\n",
      "Epoch 10/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 766.4631\n",
      "Epoch 00010: val_loss improved from 628.54114 to 588.83868, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 161us/step - loss: 998.8765 - val_loss: 588.8387\n",
      "Epoch 11/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 760.4055\n",
      "Epoch 00011: val_loss improved from 588.83868 to 541.89740, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 183us/step - loss: 933.2702 - val_loss: 541.8974\n",
      "Epoch 12/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 837.7850\n",
      "Epoch 00012: val_loss improved from 541.89740 to 491.66455, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 148us/step - loss: 859.3001 - val_loss: 491.6646\n",
      "Epoch 13/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 769.5403\n",
      "Epoch 00013: val_loss improved from 491.66455 to 442.99542, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 788.2473 - val_loss: 442.9954\n",
      "Epoch 14/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 663.8713\n",
      "Epoch 00014: val_loss improved from 442.99542 to 400.43347, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 722.7848 - val_loss: 400.4335\n",
      "Epoch 15/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 486.1408\n",
      "Epoch 00015: val_loss improved from 400.43347 to 367.39136, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 660.3022 - val_loss: 367.3914\n",
      "Epoch 16/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 898.0662\n",
      "Epoch 00016: val_loss improved from 367.39136 to 342.67850, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 618.6799 - val_loss: 342.6785\n",
      "Epoch 17/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 865.5008\n",
      "Epoch 00017: val_loss improved from 342.67850 to 325.21179, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 580.0987 - val_loss: 325.2118\n",
      "Epoch 18/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 427.6117\n",
      "Epoch 00018: val_loss improved from 325.21179 to 312.92505, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 543.4479 - val_loss: 312.9250\n",
      "Epoch 19/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 518.7274\n",
      "Epoch 00019: val_loss improved from 312.92505 to 302.46942, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 518.0127 - val_loss: 302.4694\n",
      "Epoch 20/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 459.1644\n",
      "Epoch 00020: val_loss improved from 302.46942 to 292.93158, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 160us/step - loss: 494.2470 - val_loss: 292.9316\n",
      "Epoch 21/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 507.6626\n",
      "Epoch 00021: val_loss improved from 292.93158 to 283.33163, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 473.2641 - val_loss: 283.3316\n",
      "Epoch 22/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 483.1255\n",
      "Epoch 00022: val_loss improved from 283.33163 to 273.70947, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 451.7622 - val_loss: 273.7095\n",
      "Epoch 23/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 506.4307\n",
      "Epoch 00023: val_loss improved from 273.70947 to 264.37469, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 141us/step - loss: 432.2099 - val_loss: 264.3747\n",
      "Epoch 24/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 412.3498\n",
      "Epoch 00024: val_loss improved from 264.37469 to 255.38663, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 412.0118 - val_loss: 255.3866\n",
      "Epoch 25/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 502.8481\n",
      "Epoch 00025: val_loss improved from 255.38663 to 246.61307, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 155us/step - loss: 396.1040 - val_loss: 246.6131\n",
      "Epoch 26/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 466.4313\n",
      "Epoch 00026: val_loss improved from 246.61307 to 238.25842, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 379.1531 - val_loss: 238.2584\n",
      "Epoch 27/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 340.0890\n",
      "Epoch 00027: val_loss improved from 238.25842 to 230.19905, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 362.8948 - val_loss: 230.1991\n",
      "Epoch 28/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 429.9160\n",
      "Epoch 00028: val_loss improved from 230.19905 to 222.23216, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 161us/step - loss: 348.6748 - val_loss: 222.2322\n",
      "Epoch 29/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 350.9949\n",
      "Epoch 00029: val_loss improved from 222.23216 to 214.51720, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 220us/step - loss: 333.9874 - val_loss: 214.5172\n",
      "Epoch 30/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 374.4118\n",
      "Epoch 00030: val_loss improved from 214.51720 to 206.97545, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 320.7088 - val_loss: 206.9754\n",
      "Epoch 31/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 305.0776\n",
      "Epoch 00031: val_loss improved from 206.97545 to 199.59911, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 158us/step - loss: 307.8605 - val_loss: 199.5991\n",
      "Epoch 32/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 242.1504\n",
      "Epoch 00032: val_loss improved from 199.59911 to 192.64288, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 295.1103 - val_loss: 192.6429\n",
      "Epoch 33/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 269.8819\n",
      "Epoch 00033: val_loss improved from 192.64288 to 186.06665, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 129us/step - loss: 283.3483 - val_loss: 186.0667\n",
      "Epoch 34/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 265.6841\n",
      "Epoch 00034: val_loss improved from 186.06665 to 179.72681, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 125us/step - loss: 272.0634 - val_loss: 179.7268\n",
      "Epoch 35/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 282.1287\n",
      "Epoch 00035: val_loss improved from 179.72681 to 173.50716, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 122us/step - loss: 261.6580 - val_loss: 173.5072\n",
      "Epoch 36/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 215.3121\n",
      "Epoch 00036: val_loss improved from 173.50716 to 167.60486, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 250.5568 - val_loss: 167.6049\n",
      "Epoch 37/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 280.1606\n",
      "Epoch 00037: val_loss improved from 167.60486 to 161.85107, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 241.1704 - val_loss: 161.8511\n",
      "Epoch 38/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 240.9215\n",
      "Epoch 00038: val_loss improved from 161.85107 to 156.49452, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 231.4868 - val_loss: 156.4945\n",
      "Epoch 39/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 227.2938\n",
      "Epoch 00039: val_loss improved from 156.49452 to 151.39018, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 222.3162 - val_loss: 151.3902\n",
      "Epoch 40/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 159.1290\n",
      "Epoch 00040: val_loss improved from 151.39018 to 146.65811, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 135us/step - loss: 212.7104 - val_loss: 146.6581\n",
      "Epoch 41/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 203.4579\n",
      "Epoch 00041: val_loss improved from 146.65811 to 142.26514, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 169us/step - loss: 204.4674 - val_loss: 142.2651\n",
      "Epoch 42/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 163.8351\n",
      "Epoch 00042: val_loss improved from 142.26514 to 137.94058, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 148us/step - loss: 195.9639 - val_loss: 137.9406\n",
      "Epoch 43/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 242.5043\n",
      "Epoch 00043: val_loss improved from 137.94058 to 133.71428, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 162us/step - loss: 189.3092 - val_loss: 133.7143\n",
      "Epoch 44/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 145.7938\n",
      "Epoch 00044: val_loss improved from 133.71428 to 129.80185, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 188us/step - loss: 180.5908 - val_loss: 129.8018\n",
      "Epoch 45/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 162.2556\n",
      "Epoch 00045: val_loss improved from 129.80185 to 125.93993, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 190us/step - loss: 173.6102 - val_loss: 125.9399\n",
      "Epoch 46/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 192.6573\n",
      "Epoch 00046: val_loss improved from 125.93993 to 122.31021, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 167.4164 - val_loss: 122.3102\n",
      "Epoch 47/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 144.4007\n",
      "Epoch 00047: val_loss improved from 122.31021 to 119.02117, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 160.2786 - val_loss: 119.0212\n",
      "Epoch 48/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 176.4644\n",
      "Epoch 00048: val_loss improved from 119.02117 to 115.99638, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 154.4327 - val_loss: 115.9964\n",
      "Epoch 49/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 144.9240\n",
      "Epoch 00049: val_loss improved from 115.99638 to 113.20856, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 148.9045 - val_loss: 113.2086\n",
      "Epoch 50/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 138.7209\n",
      "Epoch 00050: val_loss improved from 113.20856 to 110.65704, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 171us/step - loss: 143.1276 - val_loss: 110.6570\n",
      "Epoch 51/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 143.7502\n",
      "Epoch 00051: val_loss improved from 110.65704 to 108.27233, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 208us/step - loss: 137.8948 - val_loss: 108.2723\n",
      "Epoch 52/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 114.8423\n",
      "Epoch 00052: val_loss improved from 108.27233 to 106.08103, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 180us/step - loss: 133.1531 - val_loss: 106.0810\n",
      "Epoch 53/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 146.9484\n",
      "Epoch 00053: val_loss improved from 106.08103 to 103.83990, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 196us/step - loss: 128.8425 - val_loss: 103.8399\n",
      "Epoch 54/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 143.9307\n",
      "Epoch 00054: val_loss improved from 103.83990 to 101.79274, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 193us/step - loss: 124.3949 - val_loss: 101.7927\n",
      "Epoch 55/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 141.4609\n",
      "Epoch 00055: val_loss improved from 101.79274 to 99.81054, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 120.4838 - val_loss: 99.8105\n",
      "Epoch 56/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 106.8124\n",
      "Epoch 00056: val_loss improved from 99.81054 to 98.09985, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 168us/step - loss: 116.3023 - val_loss: 98.0999\n",
      "Epoch 57/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 112.4277\n",
      "Epoch 00057: val_loss improved from 98.09985 to 96.32892, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 175us/step - loss: 112.8075 - val_loss: 96.3289\n",
      "Epoch 58/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 117.4348\n",
      "Epoch 00058: val_loss improved from 96.32892 to 94.72807, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 222us/step - loss: 109.3245 - val_loss: 94.7281\n",
      "Epoch 59/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 114.9387\n",
      "Epoch 00059: val_loss improved from 94.72807 to 93.34276, saving model to ./best_weight_boston.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 166us/step - loss: 106.1189 - val_loss: 93.3428\n",
      "Epoch 60/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 124.9605\n",
      "Epoch 00060: val_loss improved from 93.34276 to 91.90302, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 103.3511 - val_loss: 91.9030\n",
      "Epoch 61/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 100.8513\n",
      "Epoch 00061: val_loss improved from 91.90302 to 90.69523, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 100.2759 - val_loss: 90.6952\n",
      "Epoch 62/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 80.3053\n",
      "Epoch 00062: val_loss improved from 90.69523 to 89.58968, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 192us/step - loss: 97.5928 - val_loss: 89.5897\n",
      "Epoch 63/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 101.5234\n",
      "Epoch 00063: val_loss improved from 89.58968 to 88.57913, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 204us/step - loss: 95.3815 - val_loss: 88.5791\n",
      "Epoch 64/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 87.6888\n",
      "Epoch 00064: val_loss improved from 88.57913 to 87.61768, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 179us/step - loss: 93.1069 - val_loss: 87.6177\n",
      "Epoch 65/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 74.6829\n",
      "Epoch 00065: val_loss improved from 87.61768 to 86.69964, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 181us/step - loss: 91.0175 - val_loss: 86.6996\n",
      "Epoch 66/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 77.2613\n",
      "Epoch 00066: val_loss improved from 86.69964 to 85.87247, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 88.9570 - val_loss: 85.8725\n",
      "Epoch 67/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 81.3970\n",
      "Epoch 00067: val_loss improved from 85.87247 to 84.98103, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 137us/step - loss: 87.1439 - val_loss: 84.9810\n",
      "Epoch 68/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 93.1026\n",
      "Epoch 00068: val_loss improved from 84.98103 to 84.02757, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 85.4147 - val_loss: 84.0276\n",
      "Epoch 69/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.5667\n",
      "Epoch 00069: val_loss improved from 84.02757 to 83.13053, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 83.7054 - val_loss: 83.1305\n",
      "Epoch 70/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 108.8092\n",
      "Epoch 00070: val_loss improved from 83.13053 to 82.30360, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 128us/step - loss: 82.4188 - val_loss: 82.3036\n",
      "Epoch 71/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 70.3927\n",
      "Epoch 00071: val_loss improved from 82.30360 to 81.80060, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 172us/step - loss: 80.6085 - val_loss: 81.8006\n",
      "Epoch 72/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 78.0428\n",
      "Epoch 00072: val_loss improved from 81.80060 to 81.26911, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 196us/step - loss: 79.3131 - val_loss: 81.2691\n",
      "Epoch 73/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 75.4706\n",
      "Epoch 00073: val_loss improved from 81.26911 to 80.81602, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 180us/step - loss: 78.0628 - val_loss: 80.8160\n",
      "Epoch 74/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 79.6479\n",
      "Epoch 00074: val_loss improved from 80.81602 to 80.24850, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 173us/step - loss: 76.7351 - val_loss: 80.2485\n",
      "Epoch 75/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 67.2230\n",
      "Epoch 00075: val_loss improved from 80.24850 to 79.57196, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 75.5623 - val_loss: 79.5720\n",
      "Epoch 76/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 65.4219\n",
      "Epoch 00076: val_loss improved from 79.57196 to 78.95841, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 160us/step - loss: 74.4398 - val_loss: 78.9584\n",
      "Epoch 77/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 75.6333\n",
      "Epoch 00077: val_loss improved from 78.95841 to 78.30044, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 73.3860 - val_loss: 78.3004\n",
      "Epoch 78/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 70.2419\n",
      "Epoch 00078: val_loss improved from 78.30044 to 77.77390, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 138us/step - loss: 72.4804 - val_loss: 77.7739\n",
      "Epoch 79/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 74.6589\n",
      "Epoch 00079: val_loss improved from 77.77390 to 77.30625, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 71.5358 - val_loss: 77.3063\n",
      "Epoch 80/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 76.8722\n",
      "Epoch 00080: val_loss improved from 77.30625 to 76.98649, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 177us/step - loss: 70.6340 - val_loss: 76.9865\n",
      "Epoch 81/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 72.7310\n",
      "Epoch 00081: val_loss improved from 76.98649 to 76.74413, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 230us/step - loss: 69.7973 - val_loss: 76.7441\n",
      "Epoch 82/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.8189\n",
      "Epoch 00082: val_loss improved from 76.74413 to 76.45657, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 202us/step - loss: 69.0852 - val_loss: 76.4566\n",
      "Epoch 83/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 58.8619\n",
      "Epoch 00083: val_loss improved from 76.45657 to 76.09095, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 171us/step - loss: 68.3307 - val_loss: 76.0910\n",
      "Epoch 84/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.0433\n",
      "Epoch 00084: val_loss improved from 76.09095 to 75.61866, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 156us/step - loss: 67.5936 - val_loss: 75.6187\n",
      "Epoch 85/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 50.0280\n",
      "Epoch 00085: val_loss improved from 75.61866 to 75.31020, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 163us/step - loss: 66.9539 - val_loss: 75.3102\n",
      "Epoch 86/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 81.4753\n",
      "Epoch 00086: val_loss improved from 75.31020 to 74.78423, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 146us/step - loss: 66.3161 - val_loss: 74.7842\n",
      "Epoch 87/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 53.1834\n",
      "Epoch 00087: val_loss improved from 74.78423 to 74.37919, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 65.6997 - val_loss: 74.3792\n",
      "Epoch 88/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 57.0371\n",
      "Epoch 00088: val_loss improved from 74.37919 to 73.95006, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 188us/step - loss: 65.0743 - val_loss: 73.9501\n",
      "Epoch 89/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 60.1892\n",
      "Epoch 00089: val_loss improved from 73.95006 to 73.50053, saving model to ./best_weight_boston.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 178us/step - loss: 64.5601 - val_loss: 73.5005\n",
      "Epoch 90/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 61.4104\n",
      "Epoch 00090: val_loss improved from 73.50053 to 72.99984, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 152us/step - loss: 64.0103 - val_loss: 72.9998\n",
      "Epoch 91/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 61.8191\n",
      "Epoch 00091: val_loss improved from 72.99984 to 72.66866, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 167us/step - loss: 63.5331 - val_loss: 72.6687\n",
      "Epoch 92/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 47.8043\n",
      "Epoch 00092: val_loss improved from 72.66866 to 72.26913, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 156us/step - loss: 63.0550 - val_loss: 72.2691\n",
      "Epoch 93/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 56.5960\n",
      "Epoch 00093: val_loss improved from 72.26913 to 71.90356, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 158us/step - loss: 62.5918 - val_loss: 71.9036\n",
      "Epoch 94/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 55.6757\n",
      "Epoch 00094: val_loss improved from 71.90356 to 71.53704, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 150us/step - loss: 62.2199 - val_loss: 71.5370\n",
      "Epoch 95/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 57.0884\n",
      "Epoch 00095: val_loss improved from 71.53704 to 71.23779, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 137us/step - loss: 61.8055 - val_loss: 71.2378\n",
      "Epoch 96/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 59.5596\n",
      "Epoch 00096: val_loss improved from 71.23779 to 71.05775, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 61.4048 - val_loss: 71.0577\n",
      "Epoch 97/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 76.1394\n",
      "Epoch 00097: val_loss improved from 71.05775 to 70.90553, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 192us/step - loss: 61.0301 - val_loss: 70.9055\n",
      "Epoch 98/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 38.6778\n",
      "Epoch 00098: val_loss improved from 70.90553 to 70.75729, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 243us/step - loss: 60.7294 - val_loss: 70.7573\n",
      "Epoch 99/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 77.9007\n",
      "Epoch 00099: val_loss improved from 70.75729 to 70.45116, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 181us/step - loss: 60.3693 - val_loss: 70.4512\n",
      "Epoch 100/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 58.8941\n",
      "Epoch 00100: val_loss improved from 70.45116 to 70.21668, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 179us/step - loss: 60.0698 - val_loss: 70.2167\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"./best_weight_boston.hdf5\",verbose=1,\n",
    "                              save_best_only=True)\n",
    "hist = model1.fit(x_train,y_train,batch_size=128,epochs=100,validation_split=0.2,\n",
    "                 callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a3c7e1200d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "score_1 = model1.evaluate(x_test,y_test)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.09"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "92.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return -np.sum(np.sum(Y*np.log(y.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "Progress: 90.0% ... Training loss: 3.769 ... Validation loss: 3.768CPU times: user 2min 40s, sys: 28.8 s, total: 3min 9s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "####################\n",
    "### Set the hyperparameters in you myanswers.py file ###\n",
    "####################\n",
    "\n",
    "from nn import iterations, learning_rate,hidden_nodes, output_nodes\n",
    "\n",
    "delta = []\n",
    "N_i = X_train_matrix.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(y_train_df.index, size=128)\n",
    "    X, y = X_train_matrix[batch], y_train[batch]\n",
    "    \n",
    "    network.train(X, y)\n",
    "    \n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(X_train_matrix).T, y_train)/X_train_matrix.shape[0]\n",
    "    val_loss = MSE(network.run(X_test_flatten).T, y_test_one_hot)/X_test_flatten.shape[0]\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "output=network.run((X_test_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8200000000000003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(sum(np.argmax(output,axis=1) == y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7kop5YY03JVSyg39HxWHwqVy33+tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1818763438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(y.shape)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.78113223e-20,   2.06115362e-09,   9.99999998e-01],\n",
       "       [  4.29549702e-61,   1.23394576e-04,   9.99876605e-01],\n",
       "       [  1.12533283e-07,   1.67014200e-05,   9.99983186e-01],\n",
       "       [  8.80797078e-01,   1.19202922e-01,   2.24045327e-13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.array([[1,25,45],[-78,52,61],[75,80,91],[34,32,5]])\n",
    "c=np.exp(b)/np.sum(np.exp(b),axis=1,keepdims=True)\n",
    "np.argma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(c,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.49342711720049e+19"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 2.71828183e+00+  7.20048993e+10+   3.49342711e+19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(b),axis=1,keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from learn_rate import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 83.3% ... Training loss: 9.618 ... Validation loss: 9.652CPU times: user 1min 36s, sys: 17.4 s, total: 1min 54s\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "####################\n",
    "### Set the hyperparameters in you myanswers.py file ###\n",
    "####################\n",
    "\n",
    "from learn_rate import iterations, learning_rate,hidden_nodes, output_nodes\n",
    "\n",
    "delta = []\n",
    "N_i = X_train_matrix.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(y_train_df.index, size=128)\n",
    "    X, y = X_train_matrix[batch], y_train[batch]\n",
    "    \n",
    "    network.train(X, y)\n",
    "    \n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(X_train_matrix).T, y_train)/X_train_matrix.shape[0]\n",
    "    val_loss = MSE(network.run(X_test_flatten).T, y_test_one_hot)/X_test_flatten.shape[0]\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "output=network.run((X_test_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.100000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(sum(np.argmax(output,axis=1) == y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
